{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xGNFy_ZbcNLO",
        "12ovXS3cf_Yd"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MODULE 1 : RENDER MULTIVIEW IMAGES FROM CAD MODEL"
      ],
      "metadata": {
        "id": "xGNFy_ZbcNLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1. Import the CNOS repo\n",
        "```\n",
        "!git clone https://github.com/nv-nguyen/cnos.git\n",
        "```"
      ],
      "metadata": {
        "id": "e85MNcVSj2aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/nv-nguyen/cnos.git"
      ],
      "metadata": {
        "id": "Ozpm_jqUk5vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2. Install Dependancies\n",
        "```\n",
        "!pip install pyrender trimesh Pillow tqdm numpy matplotlib\n",
        "```"
      ],
      "metadata": {
        "id": "gixCyhbzk3y7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRUYGk34jy3S"
      },
      "outputs": [],
      "source": [
        "!pip install pyrender trimesh Pillow tqdm numpy matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3. Changes in CNOS script\n",
        "*   /content/cnos/src/utils/trimesh_utils.py ; `get_obj_diameter` function, argument passed is mesh itself not path, so no need to load mesh(change argument passed from 'mesh_path' to 'mesh' and remove the `load_mesh` statement)\n",
        "*   /content/cnos/src/poses/pyrender.py ; rename to something else :\n",
        "```\n",
        "!mv /content/cnos/src/poses/pyrender.py /content/cnos/src/poses/generate_views.py\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L7iOJwHXlXLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/cnos/src/poses/pyrender.py /content/cnos/src/poses/generate_views.py"
      ],
      "metadata": {
        "id": "BOmK4RsHmUVD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4. Changes in `generate_views.py` script\n",
        "*   At the beginning of /content/cnos/src/poses/generate_views.py, type this\n",
        "```\n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), \"../..\")))\n"
      ],
      "metadata": {
        "id": "n5Nai_Owm1lU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5. Before running we make a directory to store our model and our rendered images\n",
        "\n",
        "```\n",
        "!mkdir -p output/renders\n",
        "```\n"
      ],
      "metadata": {
        "id": "R5-RenAkoOWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p output/renders\n"
      ],
      "metadata": {
        "id": "bQz79rVon1m7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "UPLOAD YOUR CAD(.PLY) MODEL UNDER THE OUTPUT DIRECTORY (name as `model.ply` for convenience with rest of the code)\n",
        "\n",
        "or run the script below for an existing model"
      ],
      "metadata": {
        "id": "Av32UTduq-T8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O output/model.ply \"https://github.com/pitcher69/IITISOC/raw/refs/heads/main/obj_000000.ply\"\n"
      ],
      "metadata": {
        "id": "w0zFOYdAt62W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VISUALIZE MODEL"
      ],
      "metadata": {
        "id": "M61DhIuHwwp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import trimesh\n",
        "\n",
        "mesh = trimesh.load(\"output/model.ply\")\n",
        "print(\"Center (centroid):\", mesh.centroid)\n",
        "mesh.show()"
      ],
      "metadata": {
        "id": "5v4cLC1Gwzc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Centre the model to origin if it isnt"
      ],
      "metadata": {
        "id": "gEvpK1ICaRfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import trimesh\n",
        "import numpy as np\n",
        "\n",
        "def normalize_and_overwrite_model(model_path):\n",
        "    # Load mesh\n",
        "    mesh = trimesh.load_mesh(model_path)\n",
        "    print(f\"Original center: {mesh.bounding_box.centroid}\")\n",
        "\n",
        "    # Center at origin\n",
        "    centroid = mesh.bounding_box.centroid\n",
        "    mesh.apply_translation(-centroid)\n",
        "\n",
        "    # Uniform scale to unit cube\n",
        "    max_extent = np.max(mesh.bounding_box.extents)\n",
        "    if max_extent == 0:\n",
        "        raise ValueError(\"Mesh has zero size.\")\n",
        "    mesh.apply_scale(1.0 / max_extent)\n",
        "\n",
        "    # Optional: set visible color if missing\n",
        "    if mesh.visual.kind != 'vertex':\n",
        "        mesh.visual.vertex_colors = np.tile([180, 180, 180, 255], (len(mesh.vertices), 1))\n",
        "\n",
        "    # Save over the original file\n",
        "    mesh.export(model_path)\n",
        "    print(f\"New center: {mesh.bounding_box.centroid}\")\n",
        "    print(f\"Normalized and saved: {model_path}\")\n",
        "\n",
        "# Example usage\n",
        "normalize_and_overwrite_model(\"/content/output/model.ply\")\n"
      ],
      "metadata": {
        "id": "5PqKQ4claWFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6. Run the rendering script\n",
        "\n",
        "\n",
        "```\n",
        "!python path_to_rendering_script(generate_views.py)\n",
        "path_to_model\n",
        "path_to_predefined_object_pose(cnos->src->poses->predefined_poses->pick_any_OBJ_pose [level0->42,level1->162,level2->642]\n",
        "path_to_renders_folder_to_store\n",
        "0(gpu)\n",
        "False\n",
        "0.8(light_intensity)\n",
        "1.7(radius)\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "ABby01vZpIIN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/cnos/src/poses/generate_views.py /content/output/model.ply /content/cnos/src/poses/predefined_poses/obj_poses_level0.npy /content/output/renders 0 False 0.8 1.7"
      ],
      "metadata": {
        "id": "lc0fRiyapHTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate GIF to visualize the rendered images"
      ],
      "metadata": {
        "id": "1UjFsr67vlKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image, ImageSequence\n",
        "import glob\n",
        "\n",
        "image_folder = \"/content/output/renders\"\n",
        "image_files = sorted(glob.glob(f\"{image_folder}/*.png\"))\n",
        "\n",
        "# Load and convert each image to RGB (removes transparency)\n",
        "frames = []\n",
        "for img_path in image_files:\n",
        "    img = Image.open(img_path).convert(\"RGBA\")\n",
        "    # Fill transparent background with black\n",
        "    bg = Image.new(\"RGB\", img.size, (0, 0, 0))\n",
        "    bg.paste(img, mask=img.split()[3])  # Use alpha channel as mask\n",
        "    frames.append(bg)\n",
        "\n",
        "# Save as animated GIF\n",
        "if frames:\n",
        "    output_path = \"/content/output/rendered_views.gif\"\n",
        "    frames[0].save(output_path, format=\"GIF\", save_all=True,\n",
        "                   append_images=frames[1:], duration=150, loop=0)\n",
        "    print(f\"GIF saved at: {output_path}\")\n",
        "else:\n",
        "    print(\"No images found to create GIF.\")\n"
      ],
      "metadata": {
        "id": "ZABvU1kWuUVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MODULE 2A: EXTRACT FEATURES FROM RENDERED IMAGES (2D FEATURE VECTOR)"
      ],
      "metadata": {
        "id": "12ovXS3cf_Yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using DINOv2 we extract features of rendered images"
      ],
      "metadata": {
        "id": "A_6w-t3kpzK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "import torch\n",
        "\n",
        "# Load processor and model\n",
        "print(\"Loading model and processor...\")\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "model = AutoModel.from_pretrained(\"facebook/dinov2-base\").eval()\n",
        "print(\"Loaded model and processor.\")\n",
        "\n",
        "# Load all render image files\n",
        "render_files = sorted(glob.glob(\"output/renders/*.png\"))\n",
        "print(f\"Found {len(render_files)} images\")\n",
        "\n",
        "all_features = []\n",
        "\n",
        "for i, file in enumerate(render_files):\n",
        "    if not os.path.isfile(file):\n",
        "        print(f\"⚠️ Skipping non-file: {file}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        img = Image.open(file).convert(\"RGB\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not open image {file}: {e}\")\n",
        "        continue\n",
        "\n",
        "    inputs = processor(images=img, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(**inputs)\n",
        "        feats = output.last_hidden_state  # [1, tokens, dim]\n",
        "\n",
        "    if feats is not None and feats.numel() > 0:\n",
        "        all_features.append(feats[0])  # shape: [tokens, dim]\n",
        "    else:\n",
        "        print(f\"Empty features for image {i}: {file}\")\n",
        "\n",
        "# Final stacking\n",
        "if not all_features:\n",
        "    raise ValueError(\"No features extracted. Please check your images or rendering.\")\n",
        "else:\n",
        "    all_features_tensor = torch.stack(all_features, dim=0)\n",
        "    print(f\"Final feature tensor shape: {all_features_tensor.shape}\")\n",
        "    for i in range(len(all_features_tensor)):\n",
        "      print(f\"Feature tensor {i}: {all_features_tensor[i][0][:5]}\")\n"
      ],
      "metadata": {
        "id": "3z7Hg1uYp5Sf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2101b5f-7a0d-44e7-8cce-f577e6982b09"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model and processor...\n",
            "Loaded model and processor.\n",
            "Found 42 images\n",
            "Final feature tensor shape: torch.Size([42, 257, 768])\n",
            "Feature tensor 0: tensor([ 2.2824, -1.0590, -0.0379,  0.1175, -2.7167])\n",
            "Feature tensor 1: tensor([ 0.5527, -0.4434,  1.3652,  3.0403,  1.0943])\n",
            "Feature tensor 2: tensor([ 1.2104,  0.1871,  0.5861, -1.6176, -0.7504])\n",
            "Feature tensor 3: tensor([ 0.5081, -1.6777,  0.2116, -0.0277, -2.0972])\n",
            "Feature tensor 4: tensor([ 1.6865,  0.6217,  0.8571, -0.5999, -2.0089])\n",
            "Feature tensor 5: tensor([ 1.1142,  1.1273,  1.0218,  1.5027, -1.4039])\n",
            "Feature tensor 6: tensor([-0.1990, -1.0128,  0.9661, -1.4182, -1.0658])\n",
            "Feature tensor 7: tensor([ 1.3545, -1.2037,  1.5297, -0.7482, -1.4338])\n",
            "Feature tensor 8: tensor([ 0.2041,  0.2962,  0.5079, -0.8700, -1.2814])\n",
            "Feature tensor 9: tensor([ 0.4600, -0.8905,  0.8067, -0.0425, -2.1945])\n",
            "Feature tensor 10: tensor([ 1.9138,  0.8016,  1.1402, -2.0447, -2.2197])\n",
            "Feature tensor 11: tensor([ 2.2335, -0.4293,  1.7023, -0.3206, -0.7663])\n",
            "Feature tensor 12: tensor([ 1.8862, -0.4023, -0.6187, -2.0284, -0.9239])\n",
            "Feature tensor 13: tensor([ 0.1753, -1.5390,  0.8156, -1.0460, -2.6021])\n",
            "Feature tensor 14: tensor([-0.3271, -0.2881, -0.2225, -0.8587, -1.7353])\n",
            "Feature tensor 15: tensor([ 3.0591, -0.8653,  0.8131,  0.1922, -1.3480])\n",
            "Feature tensor 16: tensor([-0.4116,  0.6854, -0.2347, -2.1829, -1.7772])\n",
            "Feature tensor 17: tensor([ 1.3784, -0.6089, -0.4151, -1.7189, -1.7884])\n",
            "Feature tensor 18: tensor([ 2.7177, -1.0192, -0.4800, -1.7108, -2.3445])\n",
            "Feature tensor 19: tensor([ 0.7006,  0.5006,  0.1237, -0.3009, -2.9563])\n",
            "Feature tensor 20: tensor([ 1.0470,  0.3444,  0.7717, -0.7775, -1.2493])\n",
            "Feature tensor 21: tensor([-0.3989,  0.7439, -0.3616, -1.7595, -1.3355])\n",
            "Feature tensor 22: tensor([ 0.5953, -0.9107, -0.5636, -1.8849, -2.0804])\n",
            "Feature tensor 23: tensor([ 1.3345,  0.4377,  1.0919, -0.6867,  0.3100])\n",
            "Feature tensor 24: tensor([ 1.5065, -1.7898,  1.7439, -0.3751, -1.9883])\n",
            "Feature tensor 25: tensor([ 3.3137, -3.5469,  0.9487,  0.4351, -0.9683])\n",
            "Feature tensor 26: tensor([ 1.7820, -1.1236,  0.7245, -0.9779, -2.5021])\n",
            "Feature tensor 27: tensor([ 1.1121,  0.3545, -0.9764, -0.7278, -2.5133])\n",
            "Feature tensor 28: tensor([ 3.2664, -0.6007,  0.0364, -1.2449, -2.7690])\n",
            "Feature tensor 29: tensor([ 1.5153, -0.8461, -0.1786, -1.6095, -3.4034])\n",
            "Feature tensor 30: tensor([ 3.3219, -1.6926,  1.5757, -1.5466, -2.1289])\n",
            "Feature tensor 31: tensor([ 1.6994, -0.7491,  0.3614, -0.2690, -1.0785])\n",
            "Feature tensor 32: tensor([ 0.7670, -0.8586, -0.2440, -1.6497, -2.9726])\n",
            "Feature tensor 33: tensor([ 0.4200, -1.0278, -0.4114, -1.2493, -2.7220])\n",
            "Feature tensor 34: tensor([ 1.8438,  0.0707, -0.2826, -1.1647, -1.6166])\n",
            "Feature tensor 35: tensor([ 2.2660,  0.0607,  0.3535, -0.7344, -2.2053])\n",
            "Feature tensor 36: tensor([ 1.1768,  0.0340,  1.6005,  0.1071, -1.5460])\n",
            "Feature tensor 37: tensor([ 1.5965,  0.2984, -0.4400,  1.3377, -2.3038])\n",
            "Feature tensor 38: tensor([ 1.8174, -0.2772,  0.0072, -0.7043, -1.8974])\n",
            "Feature tensor 39: tensor([ 0.6756, -0.6363,  0.8893, -0.5354,  0.0565])\n",
            "Feature tensor 40: tensor([ 2.0912, -0.8873,  1.1133,  0.7392, -1.4052])\n",
            "Feature tensor 41: tensor([ 2.6030, -0.7389, -0.0105,  0.6119, -2.1516])\n"
          ]
        }
      ]
    }
  ]
}